{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "An Autoencocder Model for Movie Recommendation System.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahhassansh/Project_Movie_Recommendation_System_in_Pytorch/blob/master/An_Autoencocder_Model_for_Movie_Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOdOOm3SunmD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# **A Movie Recommandation System With Pytorch using Autoencoder Model**\n",
        "In this project, we try to predict the ratings a user will give to an unseen movie, based on the ratings he gave to other movies. We will use the movielens dataset. We will use AutoEncoders to create our recommandation system.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOVuiaJzusT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2krwEg-OmqOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloader Class\n",
        "\n",
        "batch_size  = 500\n",
        "\n",
        "class DatasetR(Dataset):\n",
        "\n",
        "    def __init__(self, training_set, nb_users, transform=None):\n",
        "        super(DatasetR, self).__init__()\n",
        "\n",
        "        self.training_set = training_set\n",
        "        self.nb_users = nb_users\n",
        "    def __len__(self):\n",
        "        return self.nb_users\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        sample = self.training_set[idx]\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZbVrkSRvY4m",
        "colab_type": "text"
      },
      "source": [
        "The MovieLens data contains:\n",
        "- item.data : contains informations related to a movie.\n",
        "- user.data : contains informations related to a user\n",
        "- u.data : contains the ratings given by users on different movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWJorAdnvVHY",
        "colab_type": "code",
        "outputId": "ea6ea142-f7b3-4c4a-f8af-793ff7f5155f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# Importing Ratings\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4bde66e8-ba7d-47b9-baa2-dbbe691e5ee8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-4bde66e8-ba7d-47b9-baa2-dbbe691e5ee8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving u.data to u (1).data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JjlC_Njwh2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "9ba7268e-8566-485d-a0e3-1f14d0fa588c"
      },
      "source": [
        "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp'] \n",
        "ratings = pd.read_csv('u.data', sep='\\t', names=r_cols, header = None, engine = 'python', encoding = 'latin-1') \n",
        "ratings\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3</td>\n",
              "      <td>881250949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>186</td>\n",
              "      <td>302</td>\n",
              "      <td>3</td>\n",
              "      <td>891717742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>377</td>\n",
              "      <td>1</td>\n",
              "      <td>878887116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>244</td>\n",
              "      <td>51</td>\n",
              "      <td>2</td>\n",
              "      <td>880606923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>166</td>\n",
              "      <td>346</td>\n",
              "      <td>1</td>\n",
              "      <td>886397596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>880</td>\n",
              "      <td>476</td>\n",
              "      <td>3</td>\n",
              "      <td>880175444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>716</td>\n",
              "      <td>204</td>\n",
              "      <td>5</td>\n",
              "      <td>879795543</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>276</td>\n",
              "      <td>1090</td>\n",
              "      <td>1</td>\n",
              "      <td>874795795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>13</td>\n",
              "      <td>225</td>\n",
              "      <td>2</td>\n",
              "      <td>882399156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>12</td>\n",
              "      <td>203</td>\n",
              "      <td>3</td>\n",
              "      <td>879959583</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       user_id  movie_id  rating  timestamp\n",
              "0          196       242       3  881250949\n",
              "1          186       302       3  891717742\n",
              "2           22       377       1  878887116\n",
              "3          244        51       2  880606923\n",
              "4          166       346       1  886397596\n",
              "...        ...       ...     ...        ...\n",
              "99995      880       476       3  880175444\n",
              "99996      716       204       5  879795543\n",
              "99997      276      1090       1  874795795\n",
              "99998       13       225       2  882399156\n",
              "99999       12       203       3  879959583\n",
              "\n",
              "[100000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGxi54GNyDc1",
        "colab_type": "code",
        "outputId": "27856f5e-463d-49e9-eb01-b35d169d411a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# Importing Movies\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1e8c4fbf-f4cb-4609-b1e2-979e4240fa64\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-1e8c4fbf-f4cb-4609-b1e2-979e4240fa64\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving u.item to u (1).item\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0SfWBwiyKWG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "outputId": "9cc311af-94e9-46f9-8e4d-47a9bf89a6f8"
      },
      "source": [
        "i_cols = ['movie_id', 'title' ,'release date','video release date', 'IMDb URL', 'unknown', 'Action', 'Adventure',\n",
        " 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
        " 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
        "\n",
        "\n",
        "movies = pd.read_csv('u.item',  sep='|', names=i_cols, encoding='latin-1')\n",
        "\n",
        "movies"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_id</th>\n",
              "      <th>title</th>\n",
              "      <th>release date</th>\n",
              "      <th>video release date</th>\n",
              "      <th>IMDb URL</th>\n",
              "      <th>unknown</th>\n",
              "      <th>Action</th>\n",
              "      <th>Adventure</th>\n",
              "      <th>Animation</th>\n",
              "      <th>Children's</th>\n",
              "      <th>Comedy</th>\n",
              "      <th>Crime</th>\n",
              "      <th>Documentary</th>\n",
              "      <th>Drama</th>\n",
              "      <th>Fantasy</th>\n",
              "      <th>Film-Noir</th>\n",
              "      <th>Horror</th>\n",
              "      <th>Musical</th>\n",
              "      <th>Mystery</th>\n",
              "      <th>Romance</th>\n",
              "      <th>Sci-Fi</th>\n",
              "      <th>Thriller</th>\n",
              "      <th>War</th>\n",
              "      <th>Western</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>GoldenEye (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?GoldenEye%20(...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Four Rooms (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?Four%20Rooms%...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Get Shorty (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?Get%20Shorty%...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Copycat (1995)</td>\n",
              "      <td>01-Jan-1995</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?Copycat%20(1995)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1677</th>\n",
              "      <td>1678</td>\n",
              "      <td>Mat' i syn (1997)</td>\n",
              "      <td>06-Feb-1998</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?Mat%27+i+syn+...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1678</th>\n",
              "      <td>1679</td>\n",
              "      <td>B. Monkey (1998)</td>\n",
              "      <td>06-Feb-1998</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?B%2E+Monkey+(...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1679</th>\n",
              "      <td>1680</td>\n",
              "      <td>Sliding Doors (1998)</td>\n",
              "      <td>01-Jan-1998</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/Title?Sliding+Doors+(1998)</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1680</th>\n",
              "      <td>1681</td>\n",
              "      <td>You So Crazy (1994)</td>\n",
              "      <td>01-Jan-1994</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?You%20So%20Cr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1681</th>\n",
              "      <td>1682</td>\n",
              "      <td>Scream of Stone (Schrei aus Stein) (1991)</td>\n",
              "      <td>08-Mar-1996</td>\n",
              "      <td>NaN</td>\n",
              "      <td>http://us.imdb.com/M/title-exact?Schrei%20aus%...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1682 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      movie_id                                      title  ... War  Western\n",
              "0            1                           Toy Story (1995)  ...   0        0\n",
              "1            2                           GoldenEye (1995)  ...   0        0\n",
              "2            3                          Four Rooms (1995)  ...   0        0\n",
              "3            4                          Get Shorty (1995)  ...   0        0\n",
              "4            5                             Copycat (1995)  ...   0        0\n",
              "...        ...                                        ...  ...  ..      ...\n",
              "1677      1678                          Mat' i syn (1997)  ...   0        0\n",
              "1678      1679                           B. Monkey (1998)  ...   0        0\n",
              "1679      1680                       Sliding Doors (1998)  ...   0        0\n",
              "1680      1681                        You So Crazy (1994)  ...   0        0\n",
              "1681      1682  Scream of Stone (Schrei aus Stein) (1991)  ...   0        0\n",
              "\n",
              "[1682 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kaxyeo4ynGT",
        "colab_type": "code",
        "outputId": "106a7482-d01e-458c-dfc9-51adbaec407d",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# Importing Users\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c1176ec4-7f8a-4189-9e4c-859bd368bcea\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c1176ec4-7f8a-4189-9e4c-859bd368bcea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving u.user to u (1).user\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS1SphfgzAIK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "c37eb2d6-b122-4e1c-a5f2-dd55e5a1ea14"
      },
      "source": [
        "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
        "\n",
        "users = pd.read_csv('u.user', sep='|', names=u_cols,\n",
        " encoding='latin-1')\n",
        "\n",
        "users"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>occupation</th>\n",
              "      <th>zip_code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>M</td>\n",
              "      <td>technician</td>\n",
              "      <td>85711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>53</td>\n",
              "      <td>F</td>\n",
              "      <td>other</td>\n",
              "      <td>94043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>M</td>\n",
              "      <td>writer</td>\n",
              "      <td>32067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>M</td>\n",
              "      <td>technician</td>\n",
              "      <td>43537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>33</td>\n",
              "      <td>F</td>\n",
              "      <td>other</td>\n",
              "      <td>15213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>938</th>\n",
              "      <td>939</td>\n",
              "      <td>26</td>\n",
              "      <td>F</td>\n",
              "      <td>student</td>\n",
              "      <td>33319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>939</th>\n",
              "      <td>940</td>\n",
              "      <td>32</td>\n",
              "      <td>M</td>\n",
              "      <td>administrator</td>\n",
              "      <td>02215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>940</th>\n",
              "      <td>941</td>\n",
              "      <td>20</td>\n",
              "      <td>M</td>\n",
              "      <td>student</td>\n",
              "      <td>97229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>941</th>\n",
              "      <td>942</td>\n",
              "      <td>48</td>\n",
              "      <td>F</td>\n",
              "      <td>librarian</td>\n",
              "      <td>78209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>942</th>\n",
              "      <td>943</td>\n",
              "      <td>22</td>\n",
              "      <td>M</td>\n",
              "      <td>student</td>\n",
              "      <td>77841</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>943 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     user_id  age sex     occupation zip_code\n",
              "0          1   24   M     technician    85711\n",
              "1          2   53   F          other    94043\n",
              "2          3   23   M         writer    32067\n",
              "3          4   24   M     technician    43537\n",
              "4          5   33   F          other    15213\n",
              "..       ...  ...  ..            ...      ...\n",
              "938      939   26   F        student    33319\n",
              "939      940   32   M  administrator    02215\n",
              "940      941   20   M        student    97229\n",
              "941      942   48   F      librarian    78209\n",
              "942      943   22   M        student    77841\n",
              "\n",
              "[943 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDEtc7Gl1Um8",
        "colab_type": "text"
      },
      "source": [
        "Ratings data (u.data) is the main dataset. We train the autoencoder using a subset of the data. **u1.base** is the data we use to train the model. We then test our algorithm using the **u1.test**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIr4yWBL1uh2",
        "colab_type": "code",
        "outputId": "07e66b87-c8ea-40b1-eb83-e504454fddb2",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# Importing Train Dataset (Subset of Ratings)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-078f1938-a3b0-48cf-85d3-badf4f0d8470\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-078f1938-a3b0-48cf-85d3-badf4f0d8470\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving u1.base to u1 (1).base\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78YK_cMy11y-",
        "colab_type": "code",
        "outputId": "e3d92546-fa7b-4ebb-933c-fc6735909bd1",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "# Importing Test Dataset (Subset of Ratings)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e8e0163b-d455-4a26-bd46-bbc63c9db029\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e8e0163b-d455-4a26-bd46-bbc63c9db029\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving u1.test to u1 (1).test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuJ91ukO2Y2L",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the training set and the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sMFWkXK2sjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training = ['user_id', 'movie_id', 'rating', 'timestamp' ] \n",
        "test = ['user_id', 'movie_id', 'rating', 'timestamp' ]   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTAgirYi2inu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set = pd.read_csv('u1.base', names=training, delimiter = '\\t') # Read the file\n",
        "test_set = pd.read_csv('u1.test', names=test, delimiter = '\\t') #Read the file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "025s2MyJ3Mzl",
        "colab_type": "code",
        "outputId": "0bb3b8ac-4f6f-4fdf-95dc-0183b0314954",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# Visualizing some elements of the training_set\n",
        "\n",
        "training_set[130:140]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>1</td>\n",
              "      <td>263</td>\n",
              "      <td>1</td>\n",
              "      <td>875693007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>1</td>\n",
              "      <td>268</td>\n",
              "      <td>5</td>\n",
              "      <td>875692927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>1</td>\n",
              "      <td>269</td>\n",
              "      <td>5</td>\n",
              "      <td>877482427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>1</td>\n",
              "      <td>270</td>\n",
              "      <td>5</td>\n",
              "      <td>888732827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>1</td>\n",
              "      <td>271</td>\n",
              "      <td>2</td>\n",
              "      <td>887431672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>888550871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>888551853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>888551853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>2</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>888551648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>2</td>\n",
              "      <td>100</td>\n",
              "      <td>5</td>\n",
              "      <td>888552084</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     user_id  movie_id  rating  timestamp\n",
              "130        1       263       1  875693007\n",
              "131        1       268       5  875692927\n",
              "132        1       269       5  877482427\n",
              "133        1       270       5  888732827\n",
              "134        1       271       2  887431672\n",
              "135        2         1       4  888550871\n",
              "136        2        10       2  888551853\n",
              "137        2        14       4  888551853\n",
              "138        2        25       4  888551648\n",
              "139        2       100       5  888552084"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i62wh1GAwTR9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "25bc163f-9dc5-4423-f4d0-6a8c1900d769"
      },
      "source": [
        "# Visualizing some elements of the training_set\n",
        "\n",
        "test_set[130:140]"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>1</td>\n",
              "      <td>260</td>\n",
              "      <td>1</td>\n",
              "      <td>875071713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>1</td>\n",
              "      <td>262</td>\n",
              "      <td>3</td>\n",
              "      <td>875071421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>1</td>\n",
              "      <td>264</td>\n",
              "      <td>2</td>\n",
              "      <td>875071713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>1</td>\n",
              "      <td>265</td>\n",
              "      <td>4</td>\n",
              "      <td>878542441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>1</td>\n",
              "      <td>266</td>\n",
              "      <td>1</td>\n",
              "      <td>885345728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>1</td>\n",
              "      <td>267</td>\n",
              "      <td>4</td>\n",
              "      <td>875692955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>1</td>\n",
              "      <td>272</td>\n",
              "      <td>3</td>\n",
              "      <td>887431647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>888551922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>2</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>888550871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>2</td>\n",
              "      <td>50</td>\n",
              "      <td>5</td>\n",
              "      <td>888552084</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     user_id  movie_id  rating  timestamp\n",
              "130        1       260       1  875071713\n",
              "131        1       262       3  875071421\n",
              "132        1       264       2  875071713\n",
              "133        1       265       4  878542441\n",
              "134        1       266       1  885345728\n",
              "135        1       267       4  875692955\n",
              "136        1       272       3  887431647\n",
              "137        2        13       4  888551922\n",
              "138        2        19       3  888550871\n",
              "139        2        50       5  888552084"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbUaNHG73ife",
        "colab_type": "text"
      },
      "source": [
        "Since, Pytorch expects the sata as numpy arrays therefore, we convert our dataframe into numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLjmEe-r3lnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the training and test sets into numpy arrays\n",
        "\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = np.array(test_set, dtype = 'int')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lxVoisH3uBz",
        "colab_type": "text"
      },
      "source": [
        "We need the number of users and movies. Since the data is divided into training and test set, the maximum value of id_user/id_movie is either in the training_set or in the test_set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suf95deL3vt3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "07c6ebe5-d370-488b-c3bd-f01141ebc4b9"
      },
      "source": [
        "# Getting the number of users and movies\n",
        "\n",
        "nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))\n",
        "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))\n",
        "print(\"Number of users: {}\".format(nb_users))\n",
        "print(\"Number of movies: {}\".format(nb_movies))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of users: 943\n",
            "Number of movies: 1682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4ugmKbU4DDj",
        "colab_type": "text"
      },
      "source": [
        "Then, we create the data as list of lists, expected by Pytorch. Each list of list contains the ratings that a specific user gave to the movies. If a user didn't rate a movie, we just add a 0 for that observation. We define a **convert** function which creates this list of list for us.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ST2KNTF4FWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(data):\n",
        "    new_data = [] \n",
        "    for id_users in range(1, nb_users + 1):\n",
        "        id_movies = data[:, 1][data[:, 0] == id_users] #The id of the movies rated by the current user.\n",
        "        id_ratings = data[:, 2][data[:, 0] == id_users] #The id of the ratings given by the current_user.\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies - 1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data\n",
        "\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3clTIHsW4xGO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56d672cc-6b5c-491e-abaf-19e91f9c15b8"
      },
      "source": [
        "# Converting the train data into Torch tensors\n",
        "\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "training_set.shape\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([943, 1682])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH87LF5J40kC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25c810fc-72c5-4ae5-8d47-ea8c96e6b484"
      },
      "source": [
        "# Converting the test data into Torch tensors\n",
        "\n",
        "test_set = torch.FloatTensor(test_set)\n",
        "test_set.shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([943, 1682])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AJ1J8Lw2jpu",
        "colab_type": "text"
      },
      "source": [
        "# Building the  Autoencoder (AE) Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU9ojknj42-j",
        "colab_type": "text"
      },
      "source": [
        "Then, we create an autoencoder. As can be seen below, the number of nodes of the output layer should equal the number of nodes of the input layer.\n",
        "\n",
        "\n",
        "![autoencoder](https://miro.medium.com/max/3524/1*oUbsOnYKX5DEpMOK3pH_lg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDzdAE2P5pac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The Autoencoder (AE) class.\n",
        "\n",
        "class AE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        # making the class get all the functions from the parent class nn.Module\n",
        "        super(AE, self).__init__()\n",
        "        # Creating the first encoding layer. From nb_movies to 20 outputs.\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.bn1 = nn.BatchNorm1d(20)\n",
        "        self.do1 = nn.Dropout(0.2)\n",
        "        # Creating the second encoding layer. From 20 inputs to 10 outputs\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.bn2 = nn.BatchNorm1d(10)\n",
        "        self.do2 = nn.Dropout(0.2)\n",
        "        # Creating the first decoding layer. From 10 inputs to 20 outputs\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.bn3 = nn.BatchNorm1d(20)\n",
        "        self.do3 = nn.Dropout(0.2)\n",
        "        # Creating the second hidden layer. From 20 inputs to nb_movies outputs\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        # Creating the activation fucntion. \n",
        "        self.activation = nn.Sigmoid()\n",
        "        \n",
        "        # Creating the function for forward propagation\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(self.activation(self.fc1(x)))\n",
        "        x = self.bn2(self.activation(self.fc2(x)))\n",
        "        x = self.bn3(self.activation(self.fc3(x)))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uf_V_CvD2-pc",
        "colab_type": "text"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kirx-r357kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #Creating an instance of the AE class\n",
        "\n",
        "ae = AE()\n",
        "\n",
        "datasetTrain = DatasetR(training_set = training_set, nb_users = nb_users)\n",
        "train_loader = torch.utils.data.DataLoader(datasetTrain, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "datasetTest = DatasetR(training_set = test_set, nb_users = nb_users)\n",
        "test_loader = torch.utils.data.DataLoader(datasetTest, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
        "\n",
        "# Defining a Root Mean Square Error Loss Function.\n",
        "def RMSELoss(yhat,y):\n",
        "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
        "\n",
        "criterion = RMSELoss\n",
        "# Defining the algorithm used to minimize the loss function.\n",
        "optimizer = optim.RMSprop(ae.parameters(), lr = 0.01, weight_decay = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns-gTVDk5-Nt",
        "colab_type": "code",
        "outputId": "ca6d0a3b-4d9b-4085-c1ab-75b6881ba482",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "nb_epoch = 1000\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for batch_idx, (sample) in enumerate(train_loader):\n",
        "        input = Variable(sample)\n",
        "        target = input.clone()\n",
        "        # We don't consider movies NOT rated by the current user.\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output = ae(input)\n",
        "            target.require_grad = False\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output, target)\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "            loss.backward()\n",
        "            train_loss += np.sqrt(loss.data*mean_corrector)\n",
        "            s += 1.\n",
        "            # Updating the weights of the network.\n",
        "            optimizer.step()\n",
        "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: tensor(0.1843)\n",
            "epoch: 2 loss: tensor(0.1804)\n",
            "epoch: 3 loss: tensor(0.1836)\n",
            "epoch: 4 loss: tensor(0.1805)\n",
            "epoch: 5 loss: tensor(0.1799)\n",
            "epoch: 6 loss: tensor(0.1801)\n",
            "epoch: 7 loss: tensor(0.1800)\n",
            "epoch: 8 loss: tensor(0.1845)\n",
            "epoch: 9 loss: tensor(0.1797)\n",
            "epoch: 10 loss: tensor(0.1797)\n",
            "epoch: 11 loss: tensor(0.1803)\n",
            "epoch: 12 loss: tensor(0.1802)\n",
            "epoch: 13 loss: tensor(0.1795)\n",
            "epoch: 14 loss: tensor(0.1830)\n",
            "epoch: 15 loss: tensor(0.1801)\n",
            "epoch: 16 loss: tensor(0.1834)\n",
            "epoch: 17 loss: tensor(0.1804)\n",
            "epoch: 18 loss: tensor(0.1800)\n",
            "epoch: 19 loss: tensor(0.1828)\n",
            "epoch: 20 loss: tensor(0.1819)\n",
            "epoch: 21 loss: tensor(0.1795)\n",
            "epoch: 22 loss: tensor(0.1834)\n",
            "epoch: 23 loss: tensor(0.1795)\n",
            "epoch: 24 loss: tensor(0.1830)\n",
            "epoch: 25 loss: tensor(0.1789)\n",
            "epoch: 26 loss: tensor(0.1822)\n",
            "epoch: 27 loss: tensor(0.1779)\n",
            "epoch: 28 loss: tensor(0.1821)\n",
            "epoch: 29 loss: tensor(0.1820)\n",
            "epoch: 30 loss: tensor(0.1841)\n",
            "epoch: 31 loss: tensor(0.1799)\n",
            "epoch: 32 loss: tensor(0.1796)\n",
            "epoch: 33 loss: tensor(0.1790)\n",
            "epoch: 34 loss: tensor(0.1808)\n",
            "epoch: 35 loss: tensor(0.1809)\n",
            "epoch: 36 loss: tensor(0.1823)\n",
            "epoch: 37 loss: tensor(0.1802)\n",
            "epoch: 38 loss: tensor(0.1784)\n",
            "epoch: 39 loss: tensor(0.1802)\n",
            "epoch: 40 loss: tensor(0.1815)\n",
            "epoch: 41 loss: tensor(0.1825)\n",
            "epoch: 42 loss: tensor(0.1807)\n",
            "epoch: 43 loss: tensor(0.1780)\n",
            "epoch: 44 loss: tensor(0.1786)\n",
            "epoch: 45 loss: tensor(0.1780)\n",
            "epoch: 46 loss: tensor(0.1787)\n",
            "epoch: 47 loss: tensor(0.1810)\n",
            "epoch: 48 loss: tensor(0.1820)\n",
            "epoch: 49 loss: tensor(0.1806)\n",
            "epoch: 50 loss: tensor(0.1811)\n",
            "epoch: 51 loss: tensor(0.1813)\n",
            "epoch: 52 loss: tensor(0.1773)\n",
            "epoch: 53 loss: tensor(0.1773)\n",
            "epoch: 54 loss: tensor(0.1782)\n",
            "epoch: 55 loss: tensor(0.1775)\n",
            "epoch: 56 loss: tensor(0.1779)\n",
            "epoch: 57 loss: tensor(0.1819)\n",
            "epoch: 58 loss: tensor(0.1779)\n",
            "epoch: 59 loss: tensor(0.1786)\n",
            "epoch: 60 loss: tensor(0.1793)\n",
            "epoch: 61 loss: tensor(0.1794)\n",
            "epoch: 62 loss: tensor(0.1771)\n",
            "epoch: 63 loss: tensor(0.1786)\n",
            "epoch: 64 loss: tensor(0.1781)\n",
            "epoch: 65 loss: tensor(0.1766)\n",
            "epoch: 66 loss: tensor(0.1759)\n",
            "epoch: 67 loss: tensor(0.1786)\n",
            "epoch: 68 loss: tensor(0.1772)\n",
            "epoch: 69 loss: tensor(0.1792)\n",
            "epoch: 70 loss: tensor(0.1759)\n",
            "epoch: 71 loss: tensor(0.1782)\n",
            "epoch: 72 loss: tensor(0.1782)\n",
            "epoch: 73 loss: tensor(0.1778)\n",
            "epoch: 74 loss: tensor(0.1785)\n",
            "epoch: 75 loss: tensor(0.1773)\n",
            "epoch: 76 loss: tensor(0.1773)\n",
            "epoch: 77 loss: tensor(0.1740)\n",
            "epoch: 78 loss: tensor(0.1772)\n",
            "epoch: 79 loss: tensor(0.1789)\n",
            "epoch: 80 loss: tensor(0.1793)\n",
            "epoch: 81 loss: tensor(0.1770)\n",
            "epoch: 82 loss: tensor(0.1726)\n",
            "epoch: 83 loss: tensor(0.1793)\n",
            "epoch: 84 loss: tensor(0.1748)\n",
            "epoch: 85 loss: tensor(0.1741)\n",
            "epoch: 86 loss: tensor(0.1742)\n",
            "epoch: 87 loss: tensor(0.1731)\n",
            "epoch: 88 loss: tensor(0.1743)\n",
            "epoch: 89 loss: tensor(0.1741)\n",
            "epoch: 90 loss: tensor(0.1740)\n",
            "epoch: 91 loss: tensor(0.1737)\n",
            "epoch: 92 loss: tensor(0.1733)\n",
            "epoch: 93 loss: tensor(0.1725)\n",
            "epoch: 94 loss: tensor(0.1744)\n",
            "epoch: 95 loss: tensor(0.1713)\n",
            "epoch: 96 loss: tensor(0.1727)\n",
            "epoch: 97 loss: tensor(0.1713)\n",
            "epoch: 98 loss: tensor(0.1734)\n",
            "epoch: 99 loss: tensor(0.1730)\n",
            "epoch: 100 loss: tensor(0.1696)\n",
            "epoch: 101 loss: tensor(0.1712)\n",
            "epoch: 102 loss: tensor(0.1707)\n",
            "epoch: 103 loss: tensor(0.1706)\n",
            "epoch: 104 loss: tensor(0.1704)\n",
            "epoch: 105 loss: tensor(0.1720)\n",
            "epoch: 106 loss: tensor(0.1710)\n",
            "epoch: 107 loss: tensor(0.1674)\n",
            "epoch: 108 loss: tensor(0.1678)\n",
            "epoch: 109 loss: tensor(0.1684)\n",
            "epoch: 110 loss: tensor(0.1690)\n",
            "epoch: 111 loss: tensor(0.1649)\n",
            "epoch: 112 loss: tensor(0.1658)\n",
            "epoch: 113 loss: tensor(0.1640)\n",
            "epoch: 114 loss: tensor(0.1655)\n",
            "epoch: 115 loss: tensor(0.1626)\n",
            "epoch: 116 loss: tensor(0.1623)\n",
            "epoch: 117 loss: tensor(0.1636)\n",
            "epoch: 118 loss: tensor(0.1610)\n",
            "epoch: 119 loss: tensor(0.1627)\n",
            "epoch: 120 loss: tensor(0.1614)\n",
            "epoch: 121 loss: tensor(0.1606)\n",
            "epoch: 122 loss: tensor(0.1598)\n",
            "epoch: 123 loss: tensor(0.1602)\n",
            "epoch: 124 loss: tensor(0.1570)\n",
            "epoch: 125 loss: tensor(0.1591)\n",
            "epoch: 126 loss: tensor(0.1560)\n",
            "epoch: 127 loss: tensor(0.1599)\n",
            "epoch: 128 loss: tensor(0.1545)\n",
            "epoch: 129 loss: tensor(0.1545)\n",
            "epoch: 130 loss: tensor(0.1534)\n",
            "epoch: 131 loss: tensor(0.1544)\n",
            "epoch: 132 loss: tensor(0.1539)\n",
            "epoch: 133 loss: tensor(0.1525)\n",
            "epoch: 134 loss: tensor(0.1506)\n",
            "epoch: 135 loss: tensor(0.1514)\n",
            "epoch: 136 loss: tensor(0.1487)\n",
            "epoch: 137 loss: tensor(0.1488)\n",
            "epoch: 138 loss: tensor(0.1490)\n",
            "epoch: 139 loss: tensor(0.1464)\n",
            "epoch: 140 loss: tensor(0.1453)\n",
            "epoch: 141 loss: tensor(0.1445)\n",
            "epoch: 142 loss: tensor(0.1457)\n",
            "epoch: 143 loss: tensor(0.1428)\n",
            "epoch: 144 loss: tensor(0.1440)\n",
            "epoch: 145 loss: tensor(0.1413)\n",
            "epoch: 146 loss: tensor(0.1411)\n",
            "epoch: 147 loss: tensor(0.1424)\n",
            "epoch: 148 loss: tensor(0.1407)\n",
            "epoch: 149 loss: tensor(0.1396)\n",
            "epoch: 150 loss: tensor(0.1388)\n",
            "epoch: 151 loss: tensor(0.1376)\n",
            "epoch: 152 loss: tensor(0.1376)\n",
            "epoch: 153 loss: tensor(0.1369)\n",
            "epoch: 154 loss: tensor(0.1367)\n",
            "epoch: 155 loss: tensor(0.1359)\n",
            "epoch: 156 loss: tensor(0.1343)\n",
            "epoch: 157 loss: tensor(0.1331)\n",
            "epoch: 158 loss: tensor(0.1341)\n",
            "epoch: 159 loss: tensor(0.1324)\n",
            "epoch: 160 loss: tensor(0.1318)\n",
            "epoch: 161 loss: tensor(0.1325)\n",
            "epoch: 162 loss: tensor(0.1291)\n",
            "epoch: 163 loss: tensor(0.1301)\n",
            "epoch: 164 loss: tensor(0.1292)\n",
            "epoch: 165 loss: tensor(0.1300)\n",
            "epoch: 166 loss: tensor(0.1285)\n",
            "epoch: 167 loss: tensor(0.1271)\n",
            "epoch: 168 loss: tensor(0.1276)\n",
            "epoch: 169 loss: tensor(0.1254)\n",
            "epoch: 170 loss: tensor(0.1260)\n",
            "epoch: 171 loss: tensor(0.1242)\n",
            "epoch: 172 loss: tensor(0.1243)\n",
            "epoch: 173 loss: tensor(0.1246)\n",
            "epoch: 174 loss: tensor(0.1255)\n",
            "epoch: 175 loss: tensor(0.1239)\n",
            "epoch: 176 loss: tensor(0.1242)\n",
            "epoch: 177 loss: tensor(0.1227)\n",
            "epoch: 178 loss: tensor(0.1210)\n",
            "epoch: 179 loss: tensor(0.1224)\n",
            "epoch: 180 loss: tensor(0.1222)\n",
            "epoch: 181 loss: tensor(0.1201)\n",
            "epoch: 182 loss: tensor(0.1223)\n",
            "epoch: 183 loss: tensor(0.1191)\n",
            "epoch: 184 loss: tensor(0.1205)\n",
            "epoch: 185 loss: tensor(0.1217)\n",
            "epoch: 186 loss: tensor(0.1199)\n",
            "epoch: 187 loss: tensor(0.1169)\n",
            "epoch: 188 loss: tensor(0.1191)\n",
            "epoch: 189 loss: tensor(0.1171)\n",
            "epoch: 190 loss: tensor(0.1187)\n",
            "epoch: 191 loss: tensor(0.1184)\n",
            "epoch: 192 loss: tensor(0.1161)\n",
            "epoch: 193 loss: tensor(0.1138)\n",
            "epoch: 194 loss: tensor(0.1173)\n",
            "epoch: 195 loss: tensor(0.1150)\n",
            "epoch: 196 loss: tensor(0.1129)\n",
            "epoch: 197 loss: tensor(0.1142)\n",
            "epoch: 198 loss: tensor(0.1147)\n",
            "epoch: 199 loss: tensor(0.1147)\n",
            "epoch: 200 loss: tensor(0.1136)\n",
            "epoch: 201 loss: tensor(0.1127)\n",
            "epoch: 202 loss: tensor(0.1137)\n",
            "epoch: 203 loss: tensor(0.1159)\n",
            "epoch: 204 loss: tensor(0.1142)\n",
            "epoch: 205 loss: tensor(0.1134)\n",
            "epoch: 206 loss: tensor(0.1113)\n",
            "epoch: 207 loss: tensor(0.1106)\n",
            "epoch: 208 loss: tensor(0.1115)\n",
            "epoch: 209 loss: tensor(0.1113)\n",
            "epoch: 210 loss: tensor(0.1121)\n",
            "epoch: 211 loss: tensor(0.1106)\n",
            "epoch: 212 loss: tensor(0.1108)\n",
            "epoch: 213 loss: tensor(0.1104)\n",
            "epoch: 214 loss: tensor(0.1105)\n",
            "epoch: 215 loss: tensor(0.1077)\n",
            "epoch: 216 loss: tensor(0.1090)\n",
            "epoch: 217 loss: tensor(0.1087)\n",
            "epoch: 218 loss: tensor(0.1100)\n",
            "epoch: 219 loss: tensor(0.1090)\n",
            "epoch: 220 loss: tensor(0.1073)\n",
            "epoch: 221 loss: tensor(0.1075)\n",
            "epoch: 222 loss: tensor(0.1099)\n",
            "epoch: 223 loss: tensor(0.1089)\n",
            "epoch: 224 loss: tensor(0.1101)\n",
            "epoch: 225 loss: tensor(0.1079)\n",
            "epoch: 226 loss: tensor(0.1092)\n",
            "epoch: 227 loss: tensor(0.1099)\n",
            "epoch: 228 loss: tensor(0.1077)\n",
            "epoch: 229 loss: tensor(0.1068)\n",
            "epoch: 230 loss: tensor(0.1063)\n",
            "epoch: 231 loss: tensor(0.1074)\n",
            "epoch: 232 loss: tensor(0.1052)\n",
            "epoch: 233 loss: tensor(0.1058)\n",
            "epoch: 234 loss: tensor(0.1054)\n",
            "epoch: 235 loss: tensor(0.1070)\n",
            "epoch: 236 loss: tensor(0.1047)\n",
            "epoch: 237 loss: tensor(0.1047)\n",
            "epoch: 238 loss: tensor(0.1058)\n",
            "epoch: 239 loss: tensor(0.1054)\n",
            "epoch: 240 loss: tensor(0.1041)\n",
            "epoch: 241 loss: tensor(0.1056)\n",
            "epoch: 242 loss: tensor(0.1052)\n",
            "epoch: 243 loss: tensor(0.1044)\n",
            "epoch: 244 loss: tensor(0.1042)\n",
            "epoch: 245 loss: tensor(0.1047)\n",
            "epoch: 246 loss: tensor(0.1049)\n",
            "epoch: 247 loss: tensor(0.1024)\n",
            "epoch: 248 loss: tensor(0.1042)\n",
            "epoch: 249 loss: tensor(0.1042)\n",
            "epoch: 250 loss: tensor(0.1045)\n",
            "epoch: 251 loss: tensor(0.1030)\n",
            "epoch: 252 loss: tensor(0.1042)\n",
            "epoch: 253 loss: tensor(0.1045)\n",
            "epoch: 254 loss: tensor(0.1037)\n",
            "epoch: 255 loss: tensor(0.1032)\n",
            "epoch: 256 loss: tensor(0.1030)\n",
            "epoch: 257 loss: tensor(0.1041)\n",
            "epoch: 258 loss: tensor(0.1040)\n",
            "epoch: 259 loss: tensor(0.1045)\n",
            "epoch: 260 loss: tensor(0.1024)\n",
            "epoch: 261 loss: tensor(0.1037)\n",
            "epoch: 262 loss: tensor(0.1040)\n",
            "epoch: 263 loss: tensor(0.1028)\n",
            "epoch: 264 loss: tensor(0.1030)\n",
            "epoch: 265 loss: tensor(0.1051)\n",
            "epoch: 266 loss: tensor(0.1020)\n",
            "epoch: 267 loss: tensor(0.1041)\n",
            "epoch: 268 loss: tensor(0.1045)\n",
            "epoch: 269 loss: tensor(0.1034)\n",
            "epoch: 270 loss: tensor(0.1007)\n",
            "epoch: 271 loss: tensor(0.1021)\n",
            "epoch: 272 loss: tensor(0.1018)\n",
            "epoch: 273 loss: tensor(0.1013)\n",
            "epoch: 274 loss: tensor(0.1017)\n",
            "epoch: 275 loss: tensor(0.1017)\n",
            "epoch: 276 loss: tensor(0.1013)\n",
            "epoch: 277 loss: tensor(0.1019)\n",
            "epoch: 278 loss: tensor(0.1002)\n",
            "epoch: 279 loss: tensor(0.1020)\n",
            "epoch: 280 loss: tensor(0.1010)\n",
            "epoch: 281 loss: tensor(0.1015)\n",
            "epoch: 282 loss: tensor(0.1019)\n",
            "epoch: 283 loss: tensor(0.1027)\n",
            "epoch: 284 loss: tensor(0.1004)\n",
            "epoch: 285 loss: tensor(0.0992)\n",
            "epoch: 286 loss: tensor(0.1000)\n",
            "epoch: 287 loss: tensor(0.1021)\n",
            "epoch: 288 loss: tensor(0.1006)\n",
            "epoch: 289 loss: tensor(0.1020)\n",
            "epoch: 290 loss: tensor(0.1000)\n",
            "epoch: 291 loss: tensor(0.1019)\n",
            "epoch: 292 loss: tensor(0.1007)\n",
            "epoch: 293 loss: tensor(0.0998)\n",
            "epoch: 294 loss: tensor(0.1014)\n",
            "epoch: 295 loss: tensor(0.0996)\n",
            "epoch: 296 loss: tensor(0.1007)\n",
            "epoch: 297 loss: tensor(0.1013)\n",
            "epoch: 298 loss: tensor(0.0994)\n",
            "epoch: 299 loss: tensor(0.1006)\n",
            "epoch: 300 loss: tensor(0.1007)\n",
            "epoch: 301 loss: tensor(0.1002)\n",
            "epoch: 302 loss: tensor(0.1005)\n",
            "epoch: 303 loss: tensor(0.0993)\n",
            "epoch: 304 loss: tensor(0.0989)\n",
            "epoch: 305 loss: tensor(0.0982)\n",
            "epoch: 306 loss: tensor(0.1001)\n",
            "epoch: 307 loss: tensor(0.1013)\n",
            "epoch: 308 loss: tensor(0.0989)\n",
            "epoch: 309 loss: tensor(0.1006)\n",
            "epoch: 310 loss: tensor(0.0984)\n",
            "epoch: 311 loss: tensor(0.0986)\n",
            "epoch: 312 loss: tensor(0.0979)\n",
            "epoch: 313 loss: tensor(0.1002)\n",
            "epoch: 314 loss: tensor(0.0991)\n",
            "epoch: 315 loss: tensor(0.0989)\n",
            "epoch: 316 loss: tensor(0.0989)\n",
            "epoch: 317 loss: tensor(0.1006)\n",
            "epoch: 318 loss: tensor(0.0988)\n",
            "epoch: 319 loss: tensor(0.1018)\n",
            "epoch: 320 loss: tensor(0.0979)\n",
            "epoch: 321 loss: tensor(0.0998)\n",
            "epoch: 322 loss: tensor(0.1010)\n",
            "epoch: 323 loss: tensor(0.0996)\n",
            "epoch: 324 loss: tensor(0.0983)\n",
            "epoch: 325 loss: tensor(0.0990)\n",
            "epoch: 326 loss: tensor(0.0990)\n",
            "epoch: 327 loss: tensor(0.0999)\n",
            "epoch: 328 loss: tensor(0.0998)\n",
            "epoch: 329 loss: tensor(0.0981)\n",
            "epoch: 330 loss: tensor(0.1001)\n",
            "epoch: 331 loss: tensor(0.0988)\n",
            "epoch: 332 loss: tensor(0.0981)\n",
            "epoch: 333 loss: tensor(0.0995)\n",
            "epoch: 334 loss: tensor(0.0987)\n",
            "epoch: 335 loss: tensor(0.0977)\n",
            "epoch: 336 loss: tensor(0.0980)\n",
            "epoch: 337 loss: tensor(0.0990)\n",
            "epoch: 338 loss: tensor(0.0981)\n",
            "epoch: 339 loss: tensor(0.0979)\n",
            "epoch: 340 loss: tensor(0.0973)\n",
            "epoch: 341 loss: tensor(0.0995)\n",
            "epoch: 342 loss: tensor(0.0987)\n",
            "epoch: 343 loss: tensor(0.0982)\n",
            "epoch: 344 loss: tensor(0.0984)\n",
            "epoch: 345 loss: tensor(0.0993)\n",
            "epoch: 346 loss: tensor(0.0991)\n",
            "epoch: 347 loss: tensor(0.0979)\n",
            "epoch: 348 loss: tensor(0.0977)\n",
            "epoch: 349 loss: tensor(0.1002)\n",
            "epoch: 350 loss: tensor(0.0983)\n",
            "epoch: 351 loss: tensor(0.0967)\n",
            "epoch: 352 loss: tensor(0.0978)\n",
            "epoch: 353 loss: tensor(0.0982)\n",
            "epoch: 354 loss: tensor(0.0971)\n",
            "epoch: 355 loss: tensor(0.0982)\n",
            "epoch: 356 loss: tensor(0.0983)\n",
            "epoch: 357 loss: tensor(0.0978)\n",
            "epoch: 358 loss: tensor(0.1001)\n",
            "epoch: 359 loss: tensor(0.0966)\n",
            "epoch: 360 loss: tensor(0.0986)\n",
            "epoch: 361 loss: tensor(0.0970)\n",
            "epoch: 362 loss: tensor(0.0968)\n",
            "epoch: 363 loss: tensor(0.0981)\n",
            "epoch: 364 loss: tensor(0.0969)\n",
            "epoch: 365 loss: tensor(0.0975)\n",
            "epoch: 366 loss: tensor(0.0970)\n",
            "epoch: 367 loss: tensor(0.1003)\n",
            "epoch: 368 loss: tensor(0.0963)\n",
            "epoch: 369 loss: tensor(0.0973)\n",
            "epoch: 370 loss: tensor(0.0971)\n",
            "epoch: 371 loss: tensor(0.0966)\n",
            "epoch: 372 loss: tensor(0.0995)\n",
            "epoch: 373 loss: tensor(0.0985)\n",
            "epoch: 374 loss: tensor(0.0980)\n",
            "epoch: 375 loss: tensor(0.0977)\n",
            "epoch: 376 loss: tensor(0.0974)\n",
            "epoch: 377 loss: tensor(0.0978)\n",
            "epoch: 378 loss: tensor(0.0975)\n",
            "epoch: 379 loss: tensor(0.0990)\n",
            "epoch: 380 loss: tensor(0.0986)\n",
            "epoch: 381 loss: tensor(0.0982)\n",
            "epoch: 382 loss: tensor(0.0976)\n",
            "epoch: 383 loss: tensor(0.0985)\n",
            "epoch: 384 loss: tensor(0.0963)\n",
            "epoch: 385 loss: tensor(0.0982)\n",
            "epoch: 386 loss: tensor(0.0976)\n",
            "epoch: 387 loss: tensor(0.0975)\n",
            "epoch: 388 loss: tensor(0.0960)\n",
            "epoch: 389 loss: tensor(0.0961)\n",
            "epoch: 390 loss: tensor(0.0961)\n",
            "epoch: 391 loss: tensor(0.0985)\n",
            "epoch: 392 loss: tensor(0.0973)\n",
            "epoch: 393 loss: tensor(0.0967)\n",
            "epoch: 394 loss: tensor(0.0965)\n",
            "epoch: 395 loss: tensor(0.0975)\n",
            "epoch: 396 loss: tensor(0.0974)\n",
            "epoch: 397 loss: tensor(0.0974)\n",
            "epoch: 398 loss: tensor(0.0970)\n",
            "epoch: 399 loss: tensor(0.0965)\n",
            "epoch: 400 loss: tensor(0.0954)\n",
            "epoch: 401 loss: tensor(0.0986)\n",
            "epoch: 402 loss: tensor(0.0970)\n",
            "epoch: 403 loss: tensor(0.0988)\n",
            "epoch: 404 loss: tensor(0.0966)\n",
            "epoch: 405 loss: tensor(0.0957)\n",
            "epoch: 406 loss: tensor(0.0955)\n",
            "epoch: 407 loss: tensor(0.0980)\n",
            "epoch: 408 loss: tensor(0.0958)\n",
            "epoch: 409 loss: tensor(0.0967)\n",
            "epoch: 410 loss: tensor(0.0966)\n",
            "epoch: 411 loss: tensor(0.0978)\n",
            "epoch: 412 loss: tensor(0.0954)\n",
            "epoch: 413 loss: tensor(0.0950)\n",
            "epoch: 414 loss: tensor(0.0958)\n",
            "epoch: 415 loss: tensor(0.0967)\n",
            "epoch: 416 loss: tensor(0.0956)\n",
            "epoch: 417 loss: tensor(0.0958)\n",
            "epoch: 418 loss: tensor(0.0963)\n",
            "epoch: 419 loss: tensor(0.0983)\n",
            "epoch: 420 loss: tensor(0.0970)\n",
            "epoch: 421 loss: tensor(0.0956)\n",
            "epoch: 422 loss: tensor(0.0963)\n",
            "epoch: 423 loss: tensor(0.0962)\n",
            "epoch: 424 loss: tensor(0.0969)\n",
            "epoch: 425 loss: tensor(0.0961)\n",
            "epoch: 426 loss: tensor(0.0967)\n",
            "epoch: 427 loss: tensor(0.0965)\n",
            "epoch: 428 loss: tensor(0.0962)\n",
            "epoch: 429 loss: tensor(0.0970)\n",
            "epoch: 430 loss: tensor(0.0953)\n",
            "epoch: 431 loss: tensor(0.0950)\n",
            "epoch: 432 loss: tensor(0.0958)\n",
            "epoch: 433 loss: tensor(0.0955)\n",
            "epoch: 434 loss: tensor(0.0962)\n",
            "epoch: 435 loss: tensor(0.0955)\n",
            "epoch: 436 loss: tensor(0.0966)\n",
            "epoch: 437 loss: tensor(0.0971)\n",
            "epoch: 438 loss: tensor(0.0952)\n",
            "epoch: 439 loss: tensor(0.0963)\n",
            "epoch: 440 loss: tensor(0.0964)\n",
            "epoch: 441 loss: tensor(0.0950)\n",
            "epoch: 442 loss: tensor(0.0953)\n",
            "epoch: 443 loss: tensor(0.0960)\n",
            "epoch: 444 loss: tensor(0.0978)\n",
            "epoch: 445 loss: tensor(0.0974)\n",
            "epoch: 446 loss: tensor(0.0950)\n",
            "epoch: 447 loss: tensor(0.0954)\n",
            "epoch: 448 loss: tensor(0.0955)\n",
            "epoch: 449 loss: tensor(0.0954)\n",
            "epoch: 450 loss: tensor(0.0955)\n",
            "epoch: 451 loss: tensor(0.0955)\n",
            "epoch: 452 loss: tensor(0.0960)\n",
            "epoch: 453 loss: tensor(0.0951)\n",
            "epoch: 454 loss: tensor(0.0972)\n",
            "epoch: 455 loss: tensor(0.0964)\n",
            "epoch: 456 loss: tensor(0.0960)\n",
            "epoch: 457 loss: tensor(0.0964)\n",
            "epoch: 458 loss: tensor(0.0964)\n",
            "epoch: 459 loss: tensor(0.0952)\n",
            "epoch: 460 loss: tensor(0.0968)\n",
            "epoch: 461 loss: tensor(0.0945)\n",
            "epoch: 462 loss: tensor(0.0953)\n",
            "epoch: 463 loss: tensor(0.0958)\n",
            "epoch: 464 loss: tensor(0.0946)\n",
            "epoch: 465 loss: tensor(0.0958)\n",
            "epoch: 466 loss: tensor(0.0953)\n",
            "epoch: 467 loss: tensor(0.0965)\n",
            "epoch: 468 loss: tensor(0.0955)\n",
            "epoch: 469 loss: tensor(0.0957)\n",
            "epoch: 470 loss: tensor(0.0952)\n",
            "epoch: 471 loss: tensor(0.0969)\n",
            "epoch: 472 loss: tensor(0.0959)\n",
            "epoch: 473 loss: tensor(0.0965)\n",
            "epoch: 474 loss: tensor(0.0954)\n",
            "epoch: 475 loss: tensor(0.0969)\n",
            "epoch: 476 loss: tensor(0.0949)\n",
            "epoch: 477 loss: tensor(0.0946)\n",
            "epoch: 478 loss: tensor(0.0956)\n",
            "epoch: 479 loss: tensor(0.0949)\n",
            "epoch: 480 loss: tensor(0.0941)\n",
            "epoch: 481 loss: tensor(0.0951)\n",
            "epoch: 482 loss: tensor(0.0947)\n",
            "epoch: 483 loss: tensor(0.0961)\n",
            "epoch: 484 loss: tensor(0.0959)\n",
            "epoch: 485 loss: tensor(0.0968)\n",
            "epoch: 486 loss: tensor(0.0977)\n",
            "epoch: 487 loss: tensor(0.0943)\n",
            "epoch: 488 loss: tensor(0.0959)\n",
            "epoch: 489 loss: tensor(0.0956)\n",
            "epoch: 490 loss: tensor(0.0954)\n",
            "epoch: 491 loss: tensor(0.0945)\n",
            "epoch: 492 loss: tensor(0.0952)\n",
            "epoch: 493 loss: tensor(0.0931)\n",
            "epoch: 494 loss: tensor(0.0955)\n",
            "epoch: 495 loss: tensor(0.0967)\n",
            "epoch: 496 loss: tensor(0.0939)\n",
            "epoch: 497 loss: tensor(0.0942)\n",
            "epoch: 498 loss: tensor(0.0944)\n",
            "epoch: 499 loss: tensor(0.0934)\n",
            "epoch: 500 loss: tensor(0.0946)\n",
            "epoch: 501 loss: tensor(0.0959)\n",
            "epoch: 502 loss: tensor(0.0967)\n",
            "epoch: 503 loss: tensor(0.0953)\n",
            "epoch: 504 loss: tensor(0.0948)\n",
            "epoch: 505 loss: tensor(0.0955)\n",
            "epoch: 506 loss: tensor(0.0958)\n",
            "epoch: 507 loss: tensor(0.0963)\n",
            "epoch: 508 loss: tensor(0.0943)\n",
            "epoch: 509 loss: tensor(0.0949)\n",
            "epoch: 510 loss: tensor(0.0960)\n",
            "epoch: 511 loss: tensor(0.0947)\n",
            "epoch: 512 loss: tensor(0.0942)\n",
            "epoch: 513 loss: tensor(0.0947)\n",
            "epoch: 514 loss: tensor(0.0954)\n",
            "epoch: 515 loss: tensor(0.0956)\n",
            "epoch: 516 loss: tensor(0.0950)\n",
            "epoch: 517 loss: tensor(0.0954)\n",
            "epoch: 518 loss: tensor(0.0940)\n",
            "epoch: 519 loss: tensor(0.0953)\n",
            "epoch: 520 loss: tensor(0.0947)\n",
            "epoch: 521 loss: tensor(0.0948)\n",
            "epoch: 522 loss: tensor(0.0948)\n",
            "epoch: 523 loss: tensor(0.0942)\n",
            "epoch: 524 loss: tensor(0.0960)\n",
            "epoch: 525 loss: tensor(0.0959)\n",
            "epoch: 526 loss: tensor(0.0953)\n",
            "epoch: 527 loss: tensor(0.0958)\n",
            "epoch: 528 loss: tensor(0.0940)\n",
            "epoch: 529 loss: tensor(0.0947)\n",
            "epoch: 530 loss: tensor(0.0952)\n",
            "epoch: 531 loss: tensor(0.0939)\n",
            "epoch: 532 loss: tensor(0.0937)\n",
            "epoch: 533 loss: tensor(0.0939)\n",
            "epoch: 534 loss: tensor(0.0961)\n",
            "epoch: 535 loss: tensor(0.0928)\n",
            "epoch: 536 loss: tensor(0.0956)\n",
            "epoch: 537 loss: tensor(0.0946)\n",
            "epoch: 538 loss: tensor(0.0944)\n",
            "epoch: 539 loss: tensor(0.0948)\n",
            "epoch: 540 loss: tensor(0.0937)\n",
            "epoch: 541 loss: tensor(0.0952)\n",
            "epoch: 542 loss: tensor(0.0972)\n",
            "epoch: 543 loss: tensor(0.0960)\n",
            "epoch: 544 loss: tensor(0.0932)\n",
            "epoch: 545 loss: tensor(0.0937)\n",
            "epoch: 546 loss: tensor(0.0955)\n",
            "epoch: 547 loss: tensor(0.0945)\n",
            "epoch: 548 loss: tensor(0.0930)\n",
            "epoch: 549 loss: tensor(0.0952)\n",
            "epoch: 550 loss: tensor(0.0938)\n",
            "epoch: 551 loss: tensor(0.0958)\n",
            "epoch: 552 loss: tensor(0.0943)\n",
            "epoch: 553 loss: tensor(0.0948)\n",
            "epoch: 554 loss: tensor(0.0933)\n",
            "epoch: 555 loss: tensor(0.0945)\n",
            "epoch: 556 loss: tensor(0.0958)\n",
            "epoch: 557 loss: tensor(0.0967)\n",
            "epoch: 558 loss: tensor(0.0959)\n",
            "epoch: 559 loss: tensor(0.0955)\n",
            "epoch: 560 loss: tensor(0.0957)\n",
            "epoch: 561 loss: tensor(0.0934)\n",
            "epoch: 562 loss: tensor(0.0950)\n",
            "epoch: 563 loss: tensor(0.0948)\n",
            "epoch: 564 loss: tensor(0.0954)\n",
            "epoch: 565 loss: tensor(0.0946)\n",
            "epoch: 566 loss: tensor(0.0960)\n",
            "epoch: 567 loss: tensor(0.0935)\n",
            "epoch: 568 loss: tensor(0.0928)\n",
            "epoch: 569 loss: tensor(0.0943)\n",
            "epoch: 570 loss: tensor(0.0944)\n",
            "epoch: 571 loss: tensor(0.0942)\n",
            "epoch: 572 loss: tensor(0.0946)\n",
            "epoch: 573 loss: tensor(0.0929)\n",
            "epoch: 574 loss: tensor(0.0943)\n",
            "epoch: 575 loss: tensor(0.0929)\n",
            "epoch: 576 loss: tensor(0.0938)\n",
            "epoch: 577 loss: tensor(0.0948)\n",
            "epoch: 578 loss: tensor(0.0925)\n",
            "epoch: 579 loss: tensor(0.0944)\n",
            "epoch: 580 loss: tensor(0.0925)\n",
            "epoch: 581 loss: tensor(0.0941)\n",
            "epoch: 582 loss: tensor(0.0959)\n",
            "epoch: 583 loss: tensor(0.0951)\n",
            "epoch: 584 loss: tensor(0.0929)\n",
            "epoch: 585 loss: tensor(0.0922)\n",
            "epoch: 586 loss: tensor(0.0926)\n",
            "epoch: 587 loss: tensor(0.0960)\n",
            "epoch: 588 loss: tensor(0.0944)\n",
            "epoch: 589 loss: tensor(0.0940)\n",
            "epoch: 590 loss: tensor(0.0935)\n",
            "epoch: 591 loss: tensor(0.0933)\n",
            "epoch: 592 loss: tensor(0.0954)\n",
            "epoch: 593 loss: tensor(0.0944)\n",
            "epoch: 594 loss: tensor(0.0954)\n",
            "epoch: 595 loss: tensor(0.0928)\n",
            "epoch: 596 loss: tensor(0.0934)\n",
            "epoch: 597 loss: tensor(0.0935)\n",
            "epoch: 598 loss: tensor(0.0954)\n",
            "epoch: 599 loss: tensor(0.0939)\n",
            "epoch: 600 loss: tensor(0.0933)\n",
            "epoch: 601 loss: tensor(0.0931)\n",
            "epoch: 602 loss: tensor(0.0944)\n",
            "epoch: 603 loss: tensor(0.0948)\n",
            "epoch: 604 loss: tensor(0.0953)\n",
            "epoch: 605 loss: tensor(0.0949)\n",
            "epoch: 606 loss: tensor(0.0934)\n",
            "epoch: 607 loss: tensor(0.0949)\n",
            "epoch: 608 loss: tensor(0.0944)\n",
            "epoch: 609 loss: tensor(0.0952)\n",
            "epoch: 610 loss: tensor(0.0957)\n",
            "epoch: 611 loss: tensor(0.0940)\n",
            "epoch: 612 loss: tensor(0.0935)\n",
            "epoch: 613 loss: tensor(0.0942)\n",
            "epoch: 614 loss: tensor(0.0944)\n",
            "epoch: 615 loss: tensor(0.0942)\n",
            "epoch: 616 loss: tensor(0.0949)\n",
            "epoch: 617 loss: tensor(0.0922)\n",
            "epoch: 618 loss: tensor(0.0948)\n",
            "epoch: 619 loss: tensor(0.0930)\n",
            "epoch: 620 loss: tensor(0.0947)\n",
            "epoch: 621 loss: tensor(0.0938)\n",
            "epoch: 622 loss: tensor(0.0939)\n",
            "epoch: 623 loss: tensor(0.0935)\n",
            "epoch: 624 loss: tensor(0.0936)\n",
            "epoch: 625 loss: tensor(0.0928)\n",
            "epoch: 626 loss: tensor(0.0944)\n",
            "epoch: 627 loss: tensor(0.0926)\n",
            "epoch: 628 loss: tensor(0.0943)\n",
            "epoch: 629 loss: tensor(0.0921)\n",
            "epoch: 630 loss: tensor(0.0963)\n",
            "epoch: 631 loss: tensor(0.0927)\n",
            "epoch: 632 loss: tensor(0.0928)\n",
            "epoch: 633 loss: tensor(0.0945)\n",
            "epoch: 634 loss: tensor(0.0927)\n",
            "epoch: 635 loss: tensor(0.0931)\n",
            "epoch: 636 loss: tensor(0.0937)\n",
            "epoch: 637 loss: tensor(0.0946)\n",
            "epoch: 638 loss: tensor(0.0930)\n",
            "epoch: 639 loss: tensor(0.0940)\n",
            "epoch: 640 loss: tensor(0.0947)\n",
            "epoch: 641 loss: tensor(0.0929)\n",
            "epoch: 642 loss: tensor(0.0933)\n",
            "epoch: 643 loss: tensor(0.0935)\n",
            "epoch: 644 loss: tensor(0.0944)\n",
            "epoch: 645 loss: tensor(0.0935)\n",
            "epoch: 646 loss: tensor(0.0943)\n",
            "epoch: 647 loss: tensor(0.0933)\n",
            "epoch: 648 loss: tensor(0.0937)\n",
            "epoch: 649 loss: tensor(0.0934)\n",
            "epoch: 650 loss: tensor(0.0928)\n",
            "epoch: 651 loss: tensor(0.0922)\n",
            "epoch: 652 loss: tensor(0.0922)\n",
            "epoch: 653 loss: tensor(0.0935)\n",
            "epoch: 654 loss: tensor(0.0940)\n",
            "epoch: 655 loss: tensor(0.0938)\n",
            "epoch: 656 loss: tensor(0.0940)\n",
            "epoch: 657 loss: tensor(0.0942)\n",
            "epoch: 658 loss: tensor(0.0938)\n",
            "epoch: 659 loss: tensor(0.0947)\n",
            "epoch: 660 loss: tensor(0.0932)\n",
            "epoch: 661 loss: tensor(0.0940)\n",
            "epoch: 662 loss: tensor(0.0944)\n",
            "epoch: 663 loss: tensor(0.0943)\n",
            "epoch: 664 loss: tensor(0.0945)\n",
            "epoch: 665 loss: tensor(0.0937)\n",
            "epoch: 666 loss: tensor(0.0951)\n",
            "epoch: 667 loss: tensor(0.0932)\n",
            "epoch: 668 loss: tensor(0.0946)\n",
            "epoch: 669 loss: tensor(0.0943)\n",
            "epoch: 670 loss: tensor(0.0935)\n",
            "epoch: 671 loss: tensor(0.0951)\n",
            "epoch: 672 loss: tensor(0.0923)\n",
            "epoch: 673 loss: tensor(0.0927)\n",
            "epoch: 674 loss: tensor(0.0927)\n",
            "epoch: 675 loss: tensor(0.0931)\n",
            "epoch: 676 loss: tensor(0.0939)\n",
            "epoch: 677 loss: tensor(0.0929)\n",
            "epoch: 678 loss: tensor(0.0933)\n",
            "epoch: 679 loss: tensor(0.0936)\n",
            "epoch: 680 loss: tensor(0.0941)\n",
            "epoch: 681 loss: tensor(0.0924)\n",
            "epoch: 682 loss: tensor(0.0945)\n",
            "epoch: 683 loss: tensor(0.0924)\n",
            "epoch: 684 loss: tensor(0.0934)\n",
            "epoch: 685 loss: tensor(0.0952)\n",
            "epoch: 686 loss: tensor(0.0925)\n",
            "epoch: 687 loss: tensor(0.0951)\n",
            "epoch: 688 loss: tensor(0.0937)\n",
            "epoch: 689 loss: tensor(0.0942)\n",
            "epoch: 690 loss: tensor(0.0930)\n",
            "epoch: 691 loss: tensor(0.0923)\n",
            "epoch: 692 loss: tensor(0.0931)\n",
            "epoch: 693 loss: tensor(0.0940)\n",
            "epoch: 694 loss: tensor(0.0926)\n",
            "epoch: 695 loss: tensor(0.0952)\n",
            "epoch: 696 loss: tensor(0.0928)\n",
            "epoch: 697 loss: tensor(0.0936)\n",
            "epoch: 698 loss: tensor(0.0924)\n",
            "epoch: 699 loss: tensor(0.0946)\n",
            "epoch: 700 loss: tensor(0.0925)\n",
            "epoch: 701 loss: tensor(0.0943)\n",
            "epoch: 702 loss: tensor(0.0933)\n",
            "epoch: 703 loss: tensor(0.0924)\n",
            "epoch: 704 loss: tensor(0.0923)\n",
            "epoch: 705 loss: tensor(0.0927)\n",
            "epoch: 706 loss: tensor(0.0955)\n",
            "epoch: 707 loss: tensor(0.0937)\n",
            "epoch: 708 loss: tensor(0.0944)\n",
            "epoch: 709 loss: tensor(0.0939)\n",
            "epoch: 710 loss: tensor(0.0943)\n",
            "epoch: 711 loss: tensor(0.0939)\n",
            "epoch: 712 loss: tensor(0.0929)\n",
            "epoch: 713 loss: tensor(0.0938)\n",
            "epoch: 714 loss: tensor(0.0949)\n",
            "epoch: 715 loss: tensor(0.0931)\n",
            "epoch: 716 loss: tensor(0.0943)\n",
            "epoch: 717 loss: tensor(0.0950)\n",
            "epoch: 718 loss: tensor(0.0946)\n",
            "epoch: 719 loss: tensor(0.0932)\n",
            "epoch: 720 loss: tensor(0.0939)\n",
            "epoch: 721 loss: tensor(0.0946)\n",
            "epoch: 722 loss: tensor(0.0954)\n",
            "epoch: 723 loss: tensor(0.0936)\n",
            "epoch: 724 loss: tensor(0.0936)\n",
            "epoch: 725 loss: tensor(0.0934)\n",
            "epoch: 726 loss: tensor(0.0922)\n",
            "epoch: 727 loss: tensor(0.0945)\n",
            "epoch: 728 loss: tensor(0.0941)\n",
            "epoch: 729 loss: tensor(0.0952)\n",
            "epoch: 730 loss: tensor(0.0943)\n",
            "epoch: 731 loss: tensor(0.0922)\n",
            "epoch: 732 loss: tensor(0.0946)\n",
            "epoch: 733 loss: tensor(0.0916)\n",
            "epoch: 734 loss: tensor(0.0937)\n",
            "epoch: 735 loss: tensor(0.0930)\n",
            "epoch: 736 loss: tensor(0.0931)\n",
            "epoch: 737 loss: tensor(0.0935)\n",
            "epoch: 738 loss: tensor(0.0939)\n",
            "epoch: 739 loss: tensor(0.0940)\n",
            "epoch: 740 loss: tensor(0.0957)\n",
            "epoch: 741 loss: tensor(0.0931)\n",
            "epoch: 742 loss: tensor(0.0933)\n",
            "epoch: 743 loss: tensor(0.0943)\n",
            "epoch: 744 loss: tensor(0.0931)\n",
            "epoch: 745 loss: tensor(0.0942)\n",
            "epoch: 746 loss: tensor(0.0923)\n",
            "epoch: 747 loss: tensor(0.0935)\n",
            "epoch: 748 loss: tensor(0.0934)\n",
            "epoch: 749 loss: tensor(0.0935)\n",
            "epoch: 750 loss: tensor(0.0926)\n",
            "epoch: 751 loss: tensor(0.0939)\n",
            "epoch: 752 loss: tensor(0.0940)\n",
            "epoch: 753 loss: tensor(0.0935)\n",
            "epoch: 754 loss: tensor(0.0938)\n",
            "epoch: 755 loss: tensor(0.0935)\n",
            "epoch: 756 loss: tensor(0.0924)\n",
            "epoch: 757 loss: tensor(0.0945)\n",
            "epoch: 758 loss: tensor(0.0932)\n",
            "epoch: 759 loss: tensor(0.0931)\n",
            "epoch: 760 loss: tensor(0.0938)\n",
            "epoch: 761 loss: tensor(0.0937)\n",
            "epoch: 762 loss: tensor(0.0932)\n",
            "epoch: 763 loss: tensor(0.0942)\n",
            "epoch: 764 loss: tensor(0.0949)\n",
            "epoch: 765 loss: tensor(0.0939)\n",
            "epoch: 766 loss: tensor(0.0927)\n",
            "epoch: 767 loss: tensor(0.0932)\n",
            "epoch: 768 loss: tensor(0.0969)\n",
            "epoch: 769 loss: tensor(0.0933)\n",
            "epoch: 770 loss: tensor(0.0940)\n",
            "epoch: 771 loss: tensor(0.0923)\n",
            "epoch: 772 loss: tensor(0.0942)\n",
            "epoch: 773 loss: tensor(0.0932)\n",
            "epoch: 774 loss: tensor(0.0934)\n",
            "epoch: 775 loss: tensor(0.0931)\n",
            "epoch: 776 loss: tensor(0.0939)\n",
            "epoch: 777 loss: tensor(0.0920)\n",
            "epoch: 778 loss: tensor(0.0942)\n",
            "epoch: 779 loss: tensor(0.0936)\n",
            "epoch: 780 loss: tensor(0.0920)\n",
            "epoch: 781 loss: tensor(0.0948)\n",
            "epoch: 782 loss: tensor(0.0936)\n",
            "epoch: 783 loss: tensor(0.0936)\n",
            "epoch: 784 loss: tensor(0.0929)\n",
            "epoch: 785 loss: tensor(0.0935)\n",
            "epoch: 786 loss: tensor(0.0936)\n",
            "epoch: 787 loss: tensor(0.0939)\n",
            "epoch: 788 loss: tensor(0.0922)\n",
            "epoch: 789 loss: tensor(0.0933)\n",
            "epoch: 790 loss: tensor(0.0930)\n",
            "epoch: 791 loss: tensor(0.0914)\n",
            "epoch: 792 loss: tensor(0.0945)\n",
            "epoch: 793 loss: tensor(0.0936)\n",
            "epoch: 794 loss: tensor(0.0933)\n",
            "epoch: 795 loss: tensor(0.0940)\n",
            "epoch: 796 loss: tensor(0.0926)\n",
            "epoch: 797 loss: tensor(0.0927)\n",
            "epoch: 798 loss: tensor(0.0934)\n",
            "epoch: 799 loss: tensor(0.0926)\n",
            "epoch: 800 loss: tensor(0.0955)\n",
            "epoch: 801 loss: tensor(0.0920)\n",
            "epoch: 802 loss: tensor(0.0947)\n",
            "epoch: 803 loss: tensor(0.0945)\n",
            "epoch: 804 loss: tensor(0.0936)\n",
            "epoch: 805 loss: tensor(0.0929)\n",
            "epoch: 806 loss: tensor(0.0933)\n",
            "epoch: 807 loss: tensor(0.0925)\n",
            "epoch: 808 loss: tensor(0.0926)\n",
            "epoch: 809 loss: tensor(0.0927)\n",
            "epoch: 810 loss: tensor(0.0934)\n",
            "epoch: 811 loss: tensor(0.0948)\n",
            "epoch: 812 loss: tensor(0.0931)\n",
            "epoch: 813 loss: tensor(0.0929)\n",
            "epoch: 814 loss: tensor(0.0931)\n",
            "epoch: 815 loss: tensor(0.0934)\n",
            "epoch: 816 loss: tensor(0.0936)\n",
            "epoch: 817 loss: tensor(0.0926)\n",
            "epoch: 818 loss: tensor(0.0935)\n",
            "epoch: 819 loss: tensor(0.0913)\n",
            "epoch: 820 loss: tensor(0.0923)\n",
            "epoch: 821 loss: tensor(0.0923)\n",
            "epoch: 822 loss: tensor(0.0938)\n",
            "epoch: 823 loss: tensor(0.0919)\n",
            "epoch: 824 loss: tensor(0.0938)\n",
            "epoch: 825 loss: tensor(0.0943)\n",
            "epoch: 826 loss: tensor(0.0927)\n",
            "epoch: 827 loss: tensor(0.0935)\n",
            "epoch: 828 loss: tensor(0.0919)\n",
            "epoch: 829 loss: tensor(0.0932)\n",
            "epoch: 830 loss: tensor(0.0922)\n",
            "epoch: 831 loss: tensor(0.0949)\n",
            "epoch: 832 loss: tensor(0.0931)\n",
            "epoch: 833 loss: tensor(0.0950)\n",
            "epoch: 834 loss: tensor(0.0928)\n",
            "epoch: 835 loss: tensor(0.0924)\n",
            "epoch: 836 loss: tensor(0.0937)\n",
            "epoch: 837 loss: tensor(0.0926)\n",
            "epoch: 838 loss: tensor(0.0924)\n",
            "epoch: 839 loss: tensor(0.0935)\n",
            "epoch: 840 loss: tensor(0.0923)\n",
            "epoch: 841 loss: tensor(0.0924)\n",
            "epoch: 842 loss: tensor(0.0911)\n",
            "epoch: 843 loss: tensor(0.0939)\n",
            "epoch: 844 loss: tensor(0.0931)\n",
            "epoch: 845 loss: tensor(0.0924)\n",
            "epoch: 846 loss: tensor(0.0929)\n",
            "epoch: 847 loss: tensor(0.0925)\n",
            "epoch: 848 loss: tensor(0.0941)\n",
            "epoch: 849 loss: tensor(0.0931)\n",
            "epoch: 850 loss: tensor(0.0944)\n",
            "epoch: 851 loss: tensor(0.0945)\n",
            "epoch: 852 loss: tensor(0.0934)\n",
            "epoch: 853 loss: tensor(0.0932)\n",
            "epoch: 854 loss: tensor(0.0935)\n",
            "epoch: 855 loss: tensor(0.0931)\n",
            "epoch: 856 loss: tensor(0.0945)\n",
            "epoch: 857 loss: tensor(0.0929)\n",
            "epoch: 858 loss: tensor(0.0920)\n",
            "epoch: 859 loss: tensor(0.0942)\n",
            "epoch: 860 loss: tensor(0.0925)\n",
            "epoch: 861 loss: tensor(0.0931)\n",
            "epoch: 862 loss: tensor(0.0934)\n",
            "epoch: 863 loss: tensor(0.0925)\n",
            "epoch: 864 loss: tensor(0.0926)\n",
            "epoch: 865 loss: tensor(0.0937)\n",
            "epoch: 866 loss: tensor(0.0933)\n",
            "epoch: 867 loss: tensor(0.0951)\n",
            "epoch: 868 loss: tensor(0.0949)\n",
            "epoch: 869 loss: tensor(0.0920)\n",
            "epoch: 870 loss: tensor(0.0935)\n",
            "epoch: 871 loss: tensor(0.0927)\n",
            "epoch: 872 loss: tensor(0.0919)\n",
            "epoch: 873 loss: tensor(0.0914)\n",
            "epoch: 874 loss: tensor(0.0928)\n",
            "epoch: 875 loss: tensor(0.0936)\n",
            "epoch: 876 loss: tensor(0.0934)\n",
            "epoch: 877 loss: tensor(0.0939)\n",
            "epoch: 878 loss: tensor(0.0931)\n",
            "epoch: 879 loss: tensor(0.0923)\n",
            "epoch: 880 loss: tensor(0.0929)\n",
            "epoch: 881 loss: tensor(0.0937)\n",
            "epoch: 882 loss: tensor(0.0929)\n",
            "epoch: 883 loss: tensor(0.0932)\n",
            "epoch: 884 loss: tensor(0.0942)\n",
            "epoch: 885 loss: tensor(0.0943)\n",
            "epoch: 886 loss: tensor(0.0938)\n",
            "epoch: 887 loss: tensor(0.0927)\n",
            "epoch: 888 loss: tensor(0.0932)\n",
            "epoch: 889 loss: tensor(0.0934)\n",
            "epoch: 890 loss: tensor(0.0927)\n",
            "epoch: 891 loss: tensor(0.0930)\n",
            "epoch: 892 loss: tensor(0.0929)\n",
            "epoch: 893 loss: tensor(0.0924)\n",
            "epoch: 894 loss: tensor(0.0925)\n",
            "epoch: 895 loss: tensor(0.0925)\n",
            "epoch: 896 loss: tensor(0.0924)\n",
            "epoch: 897 loss: tensor(0.0944)\n",
            "epoch: 898 loss: tensor(0.0943)\n",
            "epoch: 899 loss: tensor(0.0931)\n",
            "epoch: 900 loss: tensor(0.0933)\n",
            "epoch: 901 loss: tensor(0.0929)\n",
            "epoch: 902 loss: tensor(0.0937)\n",
            "epoch: 903 loss: tensor(0.0922)\n",
            "epoch: 904 loss: tensor(0.0928)\n",
            "epoch: 905 loss: tensor(0.0924)\n",
            "epoch: 906 loss: tensor(0.0940)\n",
            "epoch: 907 loss: tensor(0.0934)\n",
            "epoch: 908 loss: tensor(0.0942)\n",
            "epoch: 909 loss: tensor(0.0935)\n",
            "epoch: 910 loss: tensor(0.0933)\n",
            "epoch: 911 loss: tensor(0.0932)\n",
            "epoch: 912 loss: tensor(0.0924)\n",
            "epoch: 913 loss: tensor(0.0933)\n",
            "epoch: 914 loss: tensor(0.0930)\n",
            "epoch: 915 loss: tensor(0.0945)\n",
            "epoch: 916 loss: tensor(0.0943)\n",
            "epoch: 917 loss: tensor(0.0927)\n",
            "epoch: 918 loss: tensor(0.0932)\n",
            "epoch: 919 loss: tensor(0.0918)\n",
            "epoch: 920 loss: tensor(0.0927)\n",
            "epoch: 921 loss: tensor(0.0930)\n",
            "epoch: 922 loss: tensor(0.0922)\n",
            "epoch: 923 loss: tensor(0.0928)\n",
            "epoch: 924 loss: tensor(0.0925)\n",
            "epoch: 925 loss: tensor(0.0920)\n",
            "epoch: 926 loss: tensor(0.0928)\n",
            "epoch: 927 loss: tensor(0.0941)\n",
            "epoch: 928 loss: tensor(0.0922)\n",
            "epoch: 929 loss: tensor(0.0919)\n",
            "epoch: 930 loss: tensor(0.0932)\n",
            "epoch: 931 loss: tensor(0.0919)\n",
            "epoch: 932 loss: tensor(0.0938)\n",
            "epoch: 933 loss: tensor(0.0935)\n",
            "epoch: 934 loss: tensor(0.0936)\n",
            "epoch: 935 loss: tensor(0.0939)\n",
            "epoch: 936 loss: tensor(0.0911)\n",
            "epoch: 937 loss: tensor(0.0941)\n",
            "epoch: 938 loss: tensor(0.0930)\n",
            "epoch: 939 loss: tensor(0.0943)\n",
            "epoch: 940 loss: tensor(0.0926)\n",
            "epoch: 941 loss: tensor(0.0935)\n",
            "epoch: 942 loss: tensor(0.0925)\n",
            "epoch: 943 loss: tensor(0.0927)\n",
            "epoch: 944 loss: tensor(0.0931)\n",
            "epoch: 945 loss: tensor(0.0940)\n",
            "epoch: 946 loss: tensor(0.0925)\n",
            "epoch: 947 loss: tensor(0.0928)\n",
            "epoch: 948 loss: tensor(0.0935)\n",
            "epoch: 949 loss: tensor(0.0950)\n",
            "epoch: 950 loss: tensor(0.0936)\n",
            "epoch: 951 loss: tensor(0.0926)\n",
            "epoch: 952 loss: tensor(0.0925)\n",
            "epoch: 953 loss: tensor(0.0934)\n",
            "epoch: 954 loss: tensor(0.0942)\n",
            "epoch: 955 loss: tensor(0.0933)\n",
            "epoch: 956 loss: tensor(0.0915)\n",
            "epoch: 957 loss: tensor(0.0944)\n",
            "epoch: 958 loss: tensor(0.0923)\n",
            "epoch: 959 loss: tensor(0.0931)\n",
            "epoch: 960 loss: tensor(0.0940)\n",
            "epoch: 961 loss: tensor(0.0930)\n",
            "epoch: 962 loss: tensor(0.0936)\n",
            "epoch: 963 loss: tensor(0.0935)\n",
            "epoch: 964 loss: tensor(0.0939)\n",
            "epoch: 965 loss: tensor(0.0927)\n",
            "epoch: 966 loss: tensor(0.0926)\n",
            "epoch: 967 loss: tensor(0.0937)\n",
            "epoch: 968 loss: tensor(0.0925)\n",
            "epoch: 969 loss: tensor(0.0941)\n",
            "epoch: 970 loss: tensor(0.0929)\n",
            "epoch: 971 loss: tensor(0.0929)\n",
            "epoch: 972 loss: tensor(0.0932)\n",
            "epoch: 973 loss: tensor(0.0921)\n",
            "epoch: 974 loss: tensor(0.0932)\n",
            "epoch: 975 loss: tensor(0.0938)\n",
            "epoch: 976 loss: tensor(0.0925)\n",
            "epoch: 977 loss: tensor(0.0936)\n",
            "epoch: 978 loss: tensor(0.0924)\n",
            "epoch: 979 loss: tensor(0.0939)\n",
            "epoch: 980 loss: tensor(0.0936)\n",
            "epoch: 981 loss: tensor(0.0930)\n",
            "epoch: 982 loss: tensor(0.0937)\n",
            "epoch: 983 loss: tensor(0.0936)\n",
            "epoch: 984 loss: tensor(0.0922)\n",
            "epoch: 985 loss: tensor(0.0923)\n",
            "epoch: 986 loss: tensor(0.0930)\n",
            "epoch: 987 loss: tensor(0.0929)\n",
            "epoch: 988 loss: tensor(0.0938)\n",
            "epoch: 989 loss: tensor(0.0927)\n",
            "epoch: 990 loss: tensor(0.0927)\n",
            "epoch: 991 loss: tensor(0.0938)\n",
            "epoch: 992 loss: tensor(0.0941)\n",
            "epoch: 993 loss: tensor(0.0927)\n",
            "epoch: 994 loss: tensor(0.0932)\n",
            "epoch: 995 loss: tensor(0.0945)\n",
            "epoch: 996 loss: tensor(0.0944)\n",
            "epoch: 997 loss: tensor(0.0938)\n",
            "epoch: 998 loss: tensor(0.0920)\n",
            "epoch: 999 loss: tensor(0.0935)\n",
            "epoch: 1000 loss: tensor(0.0924)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQw6zDiC7RI-",
        "colab_type": "text"
      },
      "source": [
        "After a training of 1000 epochs, the train loss is 0.9124. That means for the training_set, we have : **predicted_rating - 0.0924 <= real_rating <= predicted_rating + 0.0924**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQPVO2q87ZvI",
        "colab_type": "text"
      },
      "source": [
        "# Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqYsdJjw7cbk",
        "colab_type": "code",
        "outputId": "640feb73-ec19-4ff2-8732-d78e2f6767ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "for input1,target1 in zip(train_loader,test_loader):\n",
        "    input = Variable(input1)\n",
        "    target = Variable(target1)\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = ae(input)\n",
        "        target.require_grad = False\n",
        "        output[(target == 0)] = 0\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "        test_loss += np.sqrt(loss.data*mean_corrector)\n",
        "        s += 1.\n",
        "print('test_loss: '+str(test_loss/s))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_loss: tensor(0.1380)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc9lV19Q7lXY",
        "colab_type": "text"
      },
      "source": [
        "The test_loss is 0.138. Therefore, for this specific test_set, we have : **predicted_rating - 0.138 <= real_rating <= predicted_rating + 0.138**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofrlqkBw4n2U",
        "colab_type": "text"
      },
      "source": [
        "#Few more results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZNbleMO4Y0x",
        "colab_type": "text"
      },
      "source": [
        "1) With RMSELoss + Batch_size=200 + batchnorm + Epoch 200 + LR=0.01:\n",
        "Training Loss is 0.1427, Test Loss: 0.2189\n",
        "\n",
        "2) With RMSELoss + Batch_size=500 + batchnorm + Epoch 200 + LR=0.01:\n",
        "Training Loss is 0.1354, Test Loss: 0.2056\n",
        "\n",
        "3) With RMSELoss + Batch_size=500 + batchnorm + Dropout = 0.2 + Epoch 200  + LR=0.01:\n",
        "Training Loss is 0.1470 , Test Loss: 0.2186\n",
        "\n",
        "4) With RMSELoss + Batch_size=500 + batchnorm + Epoch 500  + LR=0.01:\n",
        "Training Loss is 0.0941 , Test Loss: 0.1412\n",
        "\n",
        "**5) With RMSELoss + Batch_size=500 + batchnorm + Epoch 1000  + LR=0.01:  \n",
        "Training Loss is 0.0924 , Test Loss: 0.138**\n",
        "\n",
        "6) With RMSELoss + Batch_size=1000 + batchnorm + Epoch 500  + LR=0.001:\n",
        "Training Loss is 0.0877 , Test Loss: 0.1424"
      ]
    }
  ]
}